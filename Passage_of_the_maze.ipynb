{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "JTgg7lOyO1Iu"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import os, sys, time, datetime, json, random\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.optimizers import SGD , Adam, RMSprop\n",
        "from keras.layers import PReLU\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Основные константы:"
      ],
      "metadata": {
        "id": "KD4U5rclmq1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
        "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
        "LEFT = 0\n",
        "UP = 1\n",
        "RIGHT = 2\n",
        "DOWN = 3\n",
        "\n",
        "# Actions dictionary\n",
        "actions_dict = {\n",
        "    LEFT: 'left',\n",
        "    UP: 'up',\n",
        "    RIGHT: 'right',\n",
        "    DOWN: 'down',\n",
        "}\n",
        "\n",
        "num_actions = len(actions_dict)\n",
        "\n",
        "# Exploration factor\n",
        "epsilon = 0.1"
      ],
      "metadata": {
        "id": "2f87pMVgQU33"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Класс для работы с лабиринтом:"
      ],
      "metadata": {
        "id": "ZUxlfZtsm7Ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
        "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
        "# rat = (row, col) initial rat position (defaults to (0,0))\n",
        "\n",
        "class Qmaze(object):\n",
        "    def __init__(self, maze, rat=(0,0)):\n",
        "        self._maze = np.array(maze)\n",
        "        nrows, ncols = self._maze.shape\n",
        "        self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n",
        "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
        "        self.free_cells.remove(self.target)\n",
        "        if self._maze[self.target] == 0.0:\n",
        "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
        "        if not rat in self.free_cells:\n",
        "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
        "        self.reset(rat)\n",
        "\n",
        "    def reset(self, rat):\n",
        "        self.rat = rat\n",
        "        self.maze = np.copy(self._maze)\n",
        "        nrows, ncols = self.maze.shape\n",
        "        row, col = rat\n",
        "        self.maze[row, col] = rat_mark\n",
        "        self.state = (row, col, 'start')\n",
        "        self.min_reward = -0.5 * self.maze.size\n",
        "        self.total_reward = 0\n",
        "        self.visited = set()\n",
        "\n",
        "    def update_state(self, action):\n",
        "        nrows, ncols = self.maze.shape\n",
        "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
        "\n",
        "        if self.maze[rat_row, rat_col] > 0.0:\n",
        "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
        "\n",
        "        valid_actions = self.valid_actions()\n",
        "                \n",
        "        if not valid_actions:\n",
        "            nmode = 'blocked'\n",
        "        elif action in valid_actions:\n",
        "            nmode = 'valid'\n",
        "            if action == LEFT:\n",
        "                ncol -= 1\n",
        "            elif action == UP:\n",
        "                nrow -= 1\n",
        "            if action == RIGHT:\n",
        "                ncol += 1\n",
        "            elif action == DOWN:\n",
        "                nrow += 1\n",
        "        else:                  # invalid action, no change in rat position\n",
        "            mode = 'invalid'\n",
        "\n",
        "        # new state\n",
        "        self.state = (nrow, ncol, nmode)\n",
        "\n",
        "    def get_reward(self):\n",
        "        rat_row, rat_col, mode = self.state\n",
        "        nrows, ncols = self.maze.shape\n",
        "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
        "            return 1.0\n",
        "        if mode == 'blocked':\n",
        "            return self.min_reward - 1\n",
        "        if (rat_row, rat_col) in self.visited:\n",
        "            return -0.25\n",
        "        if mode == 'invalid':\n",
        "            return -0.75\n",
        "        if mode == 'valid':\n",
        "            return -0.04\n",
        "\n",
        "    def act(self, action):\n",
        "        self.update_state(action)\n",
        "        reward = self.get_reward()\n",
        "        self.total_reward += reward\n",
        "        status = self.game_status()\n",
        "        envstate = self.observe()\n",
        "        return envstate, reward, status\n",
        "\n",
        "    def observe(self):\n",
        "        canvas = self.draw_env()\n",
        "        envstate = canvas.reshape((1, -1))\n",
        "        return envstate\n",
        "\n",
        "    def draw_env(self):\n",
        "        canvas = np.copy(self.maze)\n",
        "        nrows, ncols = self.maze.shape\n",
        "        # clear all visual marks\n",
        "        for r in range(nrows):\n",
        "            for c in range(ncols):\n",
        "                if canvas[r,c] > 0.0:\n",
        "                    canvas[r,c] = 1.0\n",
        "        # draw the rat\n",
        "        row, col, valid = self.state\n",
        "        canvas[row, col] = rat_mark\n",
        "        return canvas\n",
        "\n",
        "    def game_status(self):\n",
        "        if self.total_reward < self.min_reward:\n",
        "            return 'lose'\n",
        "        rat_row, rat_col, mode = self.state\n",
        "        nrows, ncols = self.maze.shape\n",
        "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
        "            return 'win'\n",
        "\n",
        "        return 'not_over'\n",
        "\n",
        "    def valid_actions(self, cell=None):\n",
        "        if cell is None:\n",
        "            row, col, mode = self.state\n",
        "        else:\n",
        "            row, col = cell\n",
        "        actions = [0, 1, 2, 3]\n",
        "        nrows, ncols = self.maze.shape\n",
        "        if row == 0:\n",
        "            actions.remove(1)\n",
        "        elif row == nrows-1:\n",
        "            actions.remove(3)\n",
        "\n",
        "        if col == 0:\n",
        "            actions.remove(0)\n",
        "        elif col == ncols-1:\n",
        "            actions.remove(2)\n",
        "\n",
        "        if row>0 and self.maze[row-1,col] == 0.0:\n",
        "            actions.remove(1)\n",
        "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
        "            actions.remove(3)\n",
        "\n",
        "        if col>0 and self.maze[row,col-1] == 0.0:\n",
        "            actions.remove(0)\n",
        "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
        "            actions.remove(2)\n",
        "\n",
        "        return actions"
      ],
      "metadata": {
        "id": "fTouZQ1gQdMp"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для отрисовки лабиринта:"
      ],
      "metadata": {
        "id": "owmL5g8PnA1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show(qmaze):\n",
        "    plt.grid('on')\n",
        "    nrows, ncols = qmaze.maze.shape\n",
        "    ax = plt.gca()\n",
        "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
        "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "    canvas = np.copy(qmaze.maze)\n",
        "    for row,col in qmaze.visited:\n",
        "        canvas[row,col] = 0.6\n",
        "    rat_row, rat_col, _ = qmaze.state\n",
        "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
        "    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
        "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
        "    return img"
      ],
      "metadata": {
        "id": "kgzjM-cgQn7h"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Рассмотрим пример со следущим лабиринтом:"
      ],
      "metadata": {
        "id": "b0Zo_iA9nG64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maze = [\n",
        "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
        "    [ 1.,  0.,  1.,  1.,  1.,  0.,  1.,  1.],\n",
        "    [ 1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.],\n",
        "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
        "    [ 1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
        "    [ 1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.],\n",
        "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
        "    [ 1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
        "]"
      ],
      "metadata": {
        "id": "jN4we3U_Qq9Q"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qmaze = Qmaze(maze)\n",
        "canvas, reward, game_over = qmaze.act(DOWN)\n",
        "print(\"reward=\", reward)\n",
        "show(qmaze)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "MrWCbbYuQub2",
        "outputId": "cda34b18-ae62-40ee-e57b-c13f3f862948"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reward= -0.04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f49e3885370>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Переместим агента на несколько позиций:"
      ],
      "metadata": {
        "id": "MMaWrys1nMbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qmaze.act(DOWN)  # move down\n",
        "qmaze.act(RIGHT)  # move right\n",
        "qmaze.act(RIGHT)  # move right\n",
        "qmaze.act(RIGHT)  # move right\n",
        "qmaze.act(UP)  # move up\n",
        "show(qmaze)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "U2f54NEEQyfK",
        "outputId": "33056fb3-2f2a-4eae-e5d2-26aa54f9e109"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f49e37d8370>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция, моделирующая игру по заданной модели (обученной нейронной сети):"
      ],
      "metadata": {
        "id": "awaav2C0neoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play_game(model, qmaze, rat_cell):\n",
        "    qmaze.reset(rat_cell)\n",
        "    envstate = qmaze.observe()\n",
        "    while True:\n",
        "        prev_envstate = envstate\n",
        "        # get next action\n",
        "        q = model.predict(prev_envstate, verbose = 0)\n",
        "        action = np.argmax(q[0])\n",
        "\n",
        "        # apply action, get rewards and new state\n",
        "        envstate, reward, game_status = qmaze.act(action)\n",
        "        if game_status == 'win':\n",
        "            return True\n",
        "        elif game_status == 'lose':\n",
        "            return False"
      ],
      "metadata": {
        "id": "tn3pL6yYQ6VH"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Только для небольших лабиринтов выполняется проверка завершения (моделируются все возможные игры и проверяется, выигрывает ли наша модель во всех):"
      ],
      "metadata": {
        "id": "BCJjvf89ntPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def completion_check(model, qmaze):\n",
        "    for cell in qmaze.free_cells:\n",
        "        if not qmaze.valid_actions(cell):\n",
        "            return False\n",
        "        if not play_game(model, qmaze, cell):\n",
        "            return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "i0RegJBsQ9KQ"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Класс для хранения игровых эпизодов (или игрового опыта) в список воспоминаний:"
      ],
      "metadata": {
        "id": "8dppAzL6oGhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Experience(object):\n",
        "    def __init__(self, model, max_memory=100, discount=0.95):\n",
        "        self.model = model\n",
        "        self.max_memory = max_memory\n",
        "        self.discount = discount\n",
        "        self.memory = list()\n",
        "        self.num_actions = model.output_shape[-1]\n",
        "\n",
        "    def remember(self, episode):\n",
        "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
        "        # memory[i] = episode\n",
        "        # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
        "        self.memory.append(episode)\n",
        "        if len(self.memory) > self.max_memory:\n",
        "            del self.memory[0]\n",
        "\n",
        "    def predict(self, envstate):\n",
        "        return self.model.predict(envstate, verbose = 0)[0]\n",
        "\n",
        "    def get_data(self, data_size=10):\n",
        "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
        "        mem_size = len(self.memory)\n",
        "        data_size = min(mem_size, data_size)\n",
        "        inputs = np.zeros((data_size, env_size))\n",
        "        targets = np.zeros((data_size, self.num_actions))\n",
        "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
        "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
        "            inputs[i] = envstate\n",
        "            # There should be no target values for actions not taken.\n",
        "            targets[i] = self.predict(envstate)\n",
        "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
        "            Q_sa = np.max(self.predict(envstate_next))\n",
        "            if game_over:\n",
        "                targets[i, action] = reward\n",
        "            else:\n",
        "                # reward + gamma * max_a' Q(s', a')\n",
        "                targets[i, action] = reward + self.discount * Q_sa\n",
        "        return inputs, targets"
      ],
      "metadata": {
        "id": "6xdnwOxXRCoh"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Алгоритм обучения нашей модели нейронной сети решению лабиринта:"
      ],
      "metadata": {
        "id": "8d6q4ph8oaUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def qtrain(model, maze, **opt):\n",
        "    global epsilon\n",
        "    n_epoch = opt.get('n_epoch', 15000)\n",
        "    max_memory = opt.get('max_memory', 1000)\n",
        "    data_size = opt.get('data_size', 50)\n",
        "    weights_file = opt.get('weights_file', \"\")\n",
        "    name = opt.get('name', 'model')\n",
        "    start_time = datetime.datetime.now()\n",
        "\n",
        "    # If you want to continue training from a previous model,\n",
        "    # just supply the h5 file name to weights_file option\n",
        "    if weights_file:\n",
        "        print(\"loading weights from file: %s\" % (weights_file,))\n",
        "        model.load_weights(weights_file)\n",
        "\n",
        "    # Construct environment/game from numpy array: maze (see above)\n",
        "    qmaze = Qmaze(maze)\n",
        "\n",
        "    # Initialize experience replay object\n",
        "    experience = Experience(model, max_memory=max_memory)\n",
        "\n",
        "    win_history = []   # history of win/lose game\n",
        "    n_free_cells = len(qmaze.free_cells)\n",
        "    hsize = qmaze.maze.size//2   # history window size\n",
        "    win_rate = 0.0\n",
        "    imctr = 1\n",
        "\n",
        "    for epoch in range(n_epoch):\n",
        "        loss = 0.0\n",
        "        rat_cell = random.choice(qmaze.free_cells)\n",
        "        qmaze.reset(rat_cell)\n",
        "        game_over = False\n",
        "\n",
        "        # get initial envstate (1d flattened canvas)\n",
        "        envstate = qmaze.observe()\n",
        "\n",
        "        n_episodes = 0\n",
        "        while not game_over:\n",
        "            valid_actions = qmaze.valid_actions()\n",
        "            if not valid_actions: break\n",
        "            prev_envstate = envstate\n",
        "            # Get next action\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = random.choice(valid_actions)\n",
        "            else:\n",
        "                action = np.argmax(experience.predict(prev_envstate))\n",
        "\n",
        "            # Apply action, get reward and new envstate\n",
        "            envstate, reward, game_status = qmaze.act(action)\n",
        "            if game_status == 'win':\n",
        "                win_history.append(1)\n",
        "                game_over = True\n",
        "            elif game_status == 'lose':\n",
        "                win_history.append(0)\n",
        "                game_over = True\n",
        "            else:\n",
        "                game_over = False\n",
        "\n",
        "            # Store episode (experience)\n",
        "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
        "            experience.remember(episode)\n",
        "            n_episodes += 1\n",
        "\n",
        "            # Train neural network model\n",
        "            inputs, targets = experience.get_data(data_size=data_size)\n",
        "            h = model.fit(\n",
        "                inputs,\n",
        "                targets,\n",
        "                epochs=8,\n",
        "                batch_size=16,\n",
        "                verbose=0,\n",
        "            )\n",
        "            loss = model.evaluate(inputs, targets, verbose=0)\n",
        "\n",
        "        if len(win_history) > hsize:\n",
        "            win_rate = sum(win_history[-hsize:]) / hsize\n",
        "    \n",
        "        dt = datetime.datetime.now() - start_time\n",
        "        t = format_time(dt.total_seconds())\n",
        "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
        "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
        "        # we simply check if training has exhausted all free cells and if in all\n",
        "        # cases the agent won\n",
        "        if win_rate > 0.9 : epsilon = 0.05\n",
        "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
        "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
        "            break\n",
        "\n",
        "    # Save trained model weights and architecture, this will be used by the visualization code\n",
        "    h5file = name + \".h5\"\n",
        "    json_file = name + \".json\"\n",
        "    model.save_weights(h5file, overwrite=True)\n",
        "    with open(json_file, \"w\") as outfile:\n",
        "        json.dump(model.to_json(), outfile)\n",
        "    end_time = datetime.datetime.now()\n",
        "    dt = datetime.datetime.now() - start_time\n",
        "    seconds = dt.total_seconds()\n",
        "    t = format_time(seconds)\n",
        "    print('files: %s, %s' % (h5file, json_file))\n",
        "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
        "    return seconds\n",
        "\n",
        "# This is a small utility for printing readable time strings:\n",
        "def format_time(seconds):\n",
        "    if seconds < 400:\n",
        "        s = float(seconds)\n",
        "        return \"%.1f seconds\" % (s,)\n",
        "    elif seconds < 4000:\n",
        "        m = seconds / 60.0\n",
        "        return \"%.2f minutes\" % (m,)\n",
        "    else:\n",
        "        h = seconds / 3600.0\n",
        "        return \"%.2f hours\" % (h,)"
      ],
      "metadata": {
        "id": "7ncbDkbqRM5j"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Настройка модели:"
      ],
      "metadata": {
        "id": "n6aB3l-monrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(maze, lr=0.001):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
        "    model.add(PReLU())\n",
        "    model.add(Dense(maze.size))\n",
        "    model.add(PReLU())\n",
        "    model.add(Dense(num_actions))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model"
      ],
      "metadata": {
        "id": "_lH3o1bMRSbb"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вычислительные эксперименты:"
      ],
      "metadata": {
        "id": "3lBgrSjsouVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maze =  np.array([\n",
        "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
        "    [ 1.,  1.,  1.,  0.,  0.,  1.,  0.],\n",
        "    [ 0.,  0.,  0.,  1.,  1.,  1.,  0.],\n",
        "    [ 1.,  1.,  1.,  1.,  0.,  0.,  1.],\n",
        "    [ 1.,  0.,  0.,  0.,  1.,  1.,  1.],\n",
        "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
        "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
        "])\n",
        "\n",
        "qmaze = Qmaze(maze)\n",
        "show(qmaze)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "_xVQMd-mRdfJ",
        "outputId": "deee7bf9-352b-4bc8-e5ac-c6a1924335e0"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f49e37b8280>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(maze)\n",
        "qtrain(model, maze, epochs=1000, max_memory=8*maze.size, data_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFmtRqrlRpir",
        "outputId": "810833b7-1511-4f4e-9166-50581ce20045"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 000/14999 | Loss: 0.0015 | Episodes: 109 | Win count: 0 | Win rate: 0.000 | time: 303.6 seconds\n",
            "Epoch: 001/14999 | Loss: 0.0074 | Episodes: 105 | Win count: 0 | Win rate: 0.000 | time: 10.65 minutes\n",
            "Epoch: 002/14999 | Loss: 0.0048 | Episodes: 104 | Win count: 0 | Win rate: 0.000 | time: 16.18 minutes\n",
            "Epoch: 003/14999 | Loss: 0.0166 | Episodes: 106 | Win count: 0 | Win rate: 0.000 | time: 21.74 minutes\n",
            "Epoch: 004/14999 | Loss: 0.0016 | Episodes: 106 | Win count: 0 | Win rate: 0.000 | time: 27.35 minutes\n",
            "Epoch: 005/14999 | Loss: 0.0062 | Episodes: 104 | Win count: 0 | Win rate: 0.000 | time: 32.85 minutes\n",
            "Epoch: 006/14999 | Loss: 0.0016 | Episodes: 83 | Win count: 1 | Win rate: 0.000 | time: 37.22 minutes\n",
            "Epoch: 007/14999 | Loss: 0.0050 | Episodes: 16 | Win count: 2 | Win rate: 0.000 | time: 38.08 minutes\n",
            "Epoch: 008/14999 | Loss: 0.0071 | Episodes: 103 | Win count: 2 | Win rate: 0.000 | time: 43.48 minutes\n",
            "Epoch: 009/14999 | Loss: 0.0060 | Episodes: 84 | Win count: 3 | Win rate: 0.000 | time: 47.90 minutes\n",
            "Epoch: 010/14999 | Loss: 0.0027 | Episodes: 18 | Win count: 4 | Win rate: 0.000 | time: 48.87 minutes\n",
            "Epoch: 011/14999 | Loss: 0.0063 | Episodes: 98 | Win count: 5 | Win rate: 0.000 | time: 54.09 minutes\n",
            "Epoch: 012/14999 | Loss: 0.0020 | Episodes: 102 | Win count: 5 | Win rate: 0.000 | time: 59.50 minutes\n",
            "Epoch: 013/14999 | Loss: 0.0023 | Episodes: 1 | Win count: 6 | Win rate: 0.000 | time: 59.55 minutes\n",
            "Epoch: 014/14999 | Loss: 0.0037 | Episodes: 104 | Win count: 6 | Win rate: 0.000 | time: 65.03 minutes\n",
            "Epoch: 015/14999 | Loss: 0.0019 | Episodes: 12 | Win count: 7 | Win rate: 0.000 | time: 65.68 minutes\n",
            "Epoch: 016/14999 | Loss: 0.0014 | Episodes: 2 | Win count: 8 | Win rate: 0.000 | time: 65.79 minutes\n",
            "Epoch: 017/14999 | Loss: 0.0104 | Episodes: 13 | Win count: 9 | Win rate: 0.000 | time: 66.48 minutes\n",
            "Epoch: 018/14999 | Loss: 0.0095 | Episodes: 4 | Win count: 10 | Win rate: 0.000 | time: 1.11 hours\n",
            "Epoch: 019/14999 | Loss: 0.0023 | Episodes: 8 | Win count: 11 | Win rate: 0.000 | time: 1.12 hours\n",
            "Epoch: 020/14999 | Loss: 0.0164 | Episodes: 2 | Win count: 12 | Win rate: 0.000 | time: 1.12 hours\n",
            "Epoch: 021/14999 | Loss: 0.0116 | Episodes: 25 | Win count: 13 | Win rate: 0.000 | time: 1.14 hours\n",
            "Epoch: 022/14999 | Loss: 0.0032 | Episodes: 104 | Win count: 13 | Win rate: 0.000 | time: 1.23 hours\n",
            "Epoch: 023/14999 | Loss: 0.0111 | Episodes: 22 | Win count: 14 | Win rate: 0.000 | time: 1.25 hours\n",
            "Epoch: 024/14999 | Loss: 0.0027 | Episodes: 2 | Win count: 15 | Win rate: 0.625 | time: 1.25 hours\n",
            "Epoch: 025/14999 | Loss: 0.0024 | Episodes: 9 | Win count: 16 | Win rate: 0.667 | time: 1.26 hours\n",
            "Epoch: 026/14999 | Loss: 0.0311 | Episodes: 15 | Win count: 17 | Win rate: 0.708 | time: 1.28 hours\n",
            "Epoch: 027/14999 | Loss: 0.0376 | Episodes: 24 | Win count: 18 | Win rate: 0.750 | time: 1.30 hours\n",
            "Epoch: 028/14999 | Loss: 0.0053 | Episodes: 114 | Win count: 19 | Win rate: 0.792 | time: 1.40 hours\n",
            "Epoch: 029/14999 | Loss: 0.0096 | Episodes: 26 | Win count: 20 | Win rate: 0.833 | time: 1.42 hours\n",
            "Epoch: 030/14999 | Loss: 0.0024 | Episodes: 104 | Win count: 20 | Win rate: 0.792 | time: 1.51 hours\n",
            "Epoch: 031/14999 | Loss: 0.0026 | Episodes: 17 | Win count: 21 | Win rate: 0.792 | time: 1.52 hours\n",
            "Epoch: 032/14999 | Loss: 0.0017 | Episodes: 16 | Win count: 22 | Win rate: 0.833 | time: 1.54 hours\n",
            "Epoch: 033/14999 | Loss: 0.0011 | Episodes: 34 | Win count: 23 | Win rate: 0.833 | time: 1.57 hours\n",
            "Epoch: 034/14999 | Loss: 0.0039 | Episodes: 1 | Win count: 24 | Win rate: 0.833 | time: 1.57 hours\n",
            "Epoch: 035/14999 | Loss: 0.0015 | Episodes: 36 | Win count: 25 | Win rate: 0.833 | time: 1.60 hours\n",
            "Epoch: 036/14999 | Loss: 0.0029 | Episodes: 13 | Win count: 26 | Win rate: 0.875 | time: 1.61 hours\n",
            "Epoch: 037/14999 | Loss: 0.0008 | Episodes: 26 | Win count: 27 | Win rate: 0.875 | time: 1.64 hours\n",
            "Epoch: 038/14999 | Loss: 0.0011 | Episodes: 10 | Win count: 28 | Win rate: 0.917 | time: 1.64 hours\n",
            "Epoch: 039/14999 | Loss: 0.0009 | Episodes: 4 | Win count: 29 | Win rate: 0.917 | time: 1.65 hours\n",
            "Epoch: 040/14999 | Loss: 0.0009 | Episodes: 1 | Win count: 30 | Win rate: 0.917 | time: 1.65 hours\n",
            "Epoch: 041/14999 | Loss: 0.0053 | Episodes: 9 | Win count: 31 | Win rate: 0.917 | time: 1.66 hours\n",
            "Epoch: 042/14999 | Loss: 0.0009 | Episodes: 13 | Win count: 32 | Win rate: 0.917 | time: 1.67 hours\n",
            "Epoch: 043/14999 | Loss: 0.0013 | Episodes: 3 | Win count: 33 | Win rate: 0.917 | time: 1.67 hours\n",
            "Epoch: 044/14999 | Loss: 0.0014 | Episodes: 31 | Win count: 34 | Win rate: 0.917 | time: 1.70 hours\n",
            "Epoch: 045/14999 | Loss: 0.0009 | Episodes: 22 | Win count: 35 | Win rate: 0.917 | time: 1.72 hours\n",
            "Epoch: 046/14999 | Loss: 0.0007 | Episodes: 9 | Win count: 36 | Win rate: 0.958 | time: 1.73 hours\n",
            "Epoch: 047/14999 | Loss: 0.0029 | Episodes: 4 | Win count: 37 | Win rate: 0.958 | time: 1.73 hours\n",
            "Epoch: 048/14999 | Loss: 0.0009 | Episodes: 20 | Win count: 38 | Win rate: 0.958 | time: 1.75 hours\n",
            "Epoch: 049/14999 | Loss: 0.0010 | Episodes: 19 | Win count: 39 | Win rate: 0.958 | time: 1.76 hours\n",
            "Epoch: 050/14999 | Loss: 0.0017 | Episodes: 25 | Win count: 40 | Win rate: 0.958 | time: 1.79 hours\n",
            "Epoch: 051/14999 | Loss: 0.0006 | Episodes: 14 | Win count: 41 | Win rate: 0.958 | time: 1.80 hours\n",
            "Epoch: 052/14999 | Loss: 0.0013 | Episodes: 26 | Win count: 42 | Win rate: 0.958 | time: 1.82 hours\n",
            "Epoch: 053/14999 | Loss: 0.0007 | Episodes: 30 | Win count: 43 | Win rate: 0.958 | time: 1.85 hours\n",
            "Epoch: 054/14999 | Loss: 0.0012 | Episodes: 10 | Win count: 44 | Win rate: 1.000 | time: 1.86 hours\n",
            "Reached 100% win rate at epoch: 54\n",
            "files: model.h5, model.json\n",
            "n_epoch: 54, max_mem: 392, data: 32, time: 1.86 hours\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6703.915097"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    }
  ]
}